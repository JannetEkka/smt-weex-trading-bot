{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SMT-WEEX Notebook 3: Evaluation & Insights (v2)\n",
        "**Project:** smt-weex-2025\n",
        "**Author:** Jannet Ekka\n",
        "\n",
        "**Updates in v2:**\n",
        "- Fixed error analysis (CatBoost 2D array issue)\n",
        "- Added class merging experiment (Institutional + CEX_Wallet -> Large_Holder)\n",
        "- Production reliability assessment\n",
        "- Trading signal recommendations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q catboost xgboost lightgbm scikit-learn pandas numpy matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "PROJECT_ID = 'smt-weex-2025'\n",
        "BUCKET = 'smt-weex-2025-models'\n",
        "\n",
        "!gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import pickle\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    classification_report, confusion_matrix, balanced_accuracy_score\n",
        ")\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Models and Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download from GCS\n",
        "!mkdir -p /content/models\n",
        "!gsutil -m cp gs://{BUCKET}/models/initial/* /content/models/\n",
        "!gsutil cp gs://{BUCKET}/data/data_splits.npz /content/\n",
        "!gsutil cp gs://{BUCKET}/data/feature_config.json /content/\n",
        "!gsutil cp gs://{BUCKET}/data/whale_features_cleaned.csv /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data splits\n",
        "splits = np.load('/content/data_splits.npz')\n",
        "X_train, y_train = splits['X_train'], splits['y_train']\n",
        "X_val, y_val = splits['X_val'], splits['y_val']\n",
        "X_test, y_test = splits['X_test'], splits['y_test']\n",
        "\n",
        "# Also load CatBoost training data\n",
        "X_train_cb = splits['X_train_cb']\n",
        "y_train_cb = splits['y_train_cb']\n",
        "\n",
        "# Load feature config\n",
        "with open('/content/feature_config.json', 'r') as f:\n",
        "    config = json.load(f)\n",
        "FEATURES = config['features']\n",
        "\n",
        "# Load original data for class merging experiment\n",
        "df = pd.read_csv('/content/whale_features_cleaned.csv')\n",
        "\n",
        "# Load label encoder\n",
        "with open('/content/models/label_encoder.pkl', 'rb') as f:\n",
        "    le = pickle.load(f)\n",
        "\n",
        "label_mapping = {i: label for i, label in enumerate(le.classes_)}\n",
        "labels = list(label_mapping.values())\n",
        "\n",
        "print(f\"Labels: {label_mapping}\")\n",
        "print(f\"Test set: {len(X_test)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load models\n",
        "models = {}\n",
        "\n",
        "# CatBoost\n",
        "models['CatBoost'] = CatBoostClassifier()\n",
        "models['CatBoost'].load_model('/content/models/catboost_whale_classifier.cbm')\n",
        "\n",
        "# Others\n",
        "with open('/content/models/xgboost_whale_classifier.pkl', 'rb') as f:\n",
        "    models['XGBoost'] = pickle.load(f)\n",
        "\n",
        "with open('/content/models/randomforest_whale_classifier.pkl', 'rb') as f:\n",
        "    models['RandomForest'] = pickle.load(f)\n",
        "\n",
        "with open('/content/models/lightgbm_whale_classifier.pkl', 'rb') as f:\n",
        "    models['LightGBM'] = pickle.load(f)\n",
        "\n",
        "print(f\"Loaded {len(models)} models\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Confusion Matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_predictions(model, X):\n",
        "    \"\"\"Get predictions, handling CatBoost's 2D output\"\"\"\n",
        "    y_pred = model.predict(X)\n",
        "    # CatBoost returns 2D array, flatten it\n",
        "    if hasattr(y_pred, 'shape') and len(y_pred.shape) > 1:\n",
        "        y_pred = y_pred.flatten()\n",
        "    return y_pred.astype(int)\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, labels, title):\n",
        "    \"\"\"Plot confusion matrix\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=labels, yticklabels=labels)\n",
        "    plt.title(title)\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot confusion matrices for all models\n",
        "confusion_matrices = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    y_pred = get_predictions(model, X_test)\n",
        "    cm = plot_confusion_matrix(y_test, y_pred, labels, f'{name} Confusion Matrix')\n",
        "    confusion_matrices[name] = cm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Per-Class Performance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed classification report for CatBoost\n",
        "y_pred_cb = get_predictions(models['CatBoost'], X_test)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CatBoost Classification Report\")\n",
        "print(\"=\" * 60)\n",
        "print(classification_report(y_test, y_pred_cb, target_names=labels, zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Per-class metrics comparison across models\n",
        "per_class_metrics = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    y_pred = get_predictions(model, X_test)\n",
        "    \n",
        "    precision = precision_score(y_test, y_pred, average=None, zero_division=0)\n",
        "    recall = recall_score(y_test, y_pred, average=None, zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred, average=None, zero_division=0)\n",
        "    \n",
        "    per_class_metrics[model_name] = {\n",
        "        'precision': dict(zip(labels, precision)),\n",
        "        'recall': dict(zip(labels, recall)),\n",
        "        'f1': dict(zip(labels, f1))\n",
        "    }\n",
        "\n",
        "# Show per-class F1 for all models\n",
        "print(\"\\n=== Per-Class F1 Scores ===\")\n",
        "f1_df = pd.DataFrame({model: metrics['f1'] for model, metrics in per_class_metrics.items()})\n",
        "f1_df['test_samples'] = [sum(y_test == le.transform([label])[0]) for label in labels]\n",
        "print(f1_df.round(4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize per-class F1 across models\n",
        "fig, ax = plt.subplots(figsize=(14, 6))\n",
        "\n",
        "x = np.arange(len(labels))\n",
        "width = 0.2\n",
        "\n",
        "for i, (model_name, metrics) in enumerate(per_class_metrics.items()):\n",
        "    f1_values = [metrics['f1'][label] for label in labels]\n",
        "    ax.bar(x + i*width, f1_values, width, label=model_name)\n",
        "\n",
        "ax.set_ylabel('F1 Score')\n",
        "ax.set_title('Per-Class F1 Score Comparison')\n",
        "ax.set_xticks(x + width * 1.5)\n",
        "ax.set_xticklabels(labels, rotation=45, ha='right')\n",
        "ax.legend()\n",
        "ax.set_ylim(0, 1)\n",
        "ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='50% threshold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Feature Importance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CatBoost feature importance\n",
        "catboost_importance = models['CatBoost'].get_feature_importance()\n",
        "importance_df = pd.DataFrame({\n",
        "    'feature': FEATURES,\n",
        "    'importance': catboost_importance\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"=== CatBoost Feature Importance (Top 15) ===\")\n",
        "print(importance_df.head(15).to_string(index=False))\n",
        "\n",
        "# WARNING about balance_eth_log dominance\n",
        "top_feature_pct = importance_df.iloc[0]['importance']\n",
        "print(f\"\\n*** WARNING: Top feature ({importance_df.iloc[0]['feature']}) has {top_feature_pct:.1f}% importance ***\")\n",
        "print(\"This suggests the model relies heavily on balance rather than behavioral patterns.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize feature importance\n",
        "plt.figure(figsize=(12, 10))\n",
        "top_n = 20\n",
        "top_features = importance_df.head(top_n)\n",
        "\n",
        "colors = ['coral' if feat == 'balance_eth_log' else 'steelblue' \n",
        "          for feat in top_features['feature']]\n",
        "\n",
        "plt.barh(range(len(top_features)), top_features['importance'].values, color=colors)\n",
        "plt.yticks(range(len(top_features)), top_features['feature'].values)\n",
        "plt.xlabel('Importance')\n",
        "plt.title(f'Top {top_n} Most Important Features (CatBoost)\\n(Orange = Dominant Feature)')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare feature importance across models\n",
        "rf_importance = models['RandomForest'].feature_importances_\n",
        "xgb_importance = models['XGBoost'].feature_importances_\n",
        "\n",
        "importance_comparison = pd.DataFrame({\n",
        "    'feature': FEATURES,\n",
        "    'CatBoost': catboost_importance / catboost_importance.sum(),\n",
        "    'RandomForest': rf_importance / rf_importance.sum(),\n",
        "    'XGBoost': xgb_importance / xgb_importance.sum()\n",
        "})\n",
        "\n",
        "importance_comparison['avg'] = importance_comparison[['CatBoost', 'RandomForest', 'XGBoost']].mean(axis=1)\n",
        "importance_comparison = importance_comparison.sort_values('avg', ascending=False)\n",
        "\n",
        "print(\"=== Consensus Top Features (All Models) ===\")\n",
        "print(importance_comparison[['feature', 'avg', 'CatBoost', 'RandomForest', 'XGBoost']].head(15).round(4).to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Error Analysis (FIXED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Error analysis for CatBoost\n",
        "y_pred_cb = get_predictions(models['CatBoost'], X_test)\n",
        "\n",
        "# Find misclassified samples\n",
        "misclassified_idx = np.where(y_test != y_pred_cb)[0]\n",
        "print(f\"Total misclassifications: {len(misclassified_idx)} / {len(y_test)} ({len(misclassified_idx)/len(y_test)*100:.1f}%)\")\n",
        "\n",
        "# Analyze confusion pairs\n",
        "confusion_pairs = []\n",
        "for idx in misclassified_idx:\n",
        "    true_label = label_mapping[y_test[idx]]\n",
        "    pred_label = label_mapping[int(y_pred_cb[idx])]  # Cast to int to fix the error\n",
        "    confusion_pairs.append((true_label, pred_label))\n",
        "\n",
        "confusion_counts = Counter(confusion_pairs)\n",
        "\n",
        "print(\"\\n=== Most Common Misclassifications (CatBoost) ===\")\n",
        "for (true_l, pred_l), count in confusion_counts.most_common(10):\n",
        "    print(f\"{true_l:15s} -> {pred_l:15s}: {count} times\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze which classes are problematic\n",
        "print(\"\\n=== Class-wise Error Analysis ===\")\n",
        "for label_idx, label in label_mapping.items():\n",
        "    mask = y_test == label_idx\n",
        "    if mask.sum() == 0:\n",
        "        continue\n",
        "    \n",
        "    correct = (y_pred_cb[mask] == label_idx).sum()\n",
        "    total = mask.sum()\n",
        "    accuracy = correct / total * 100\n",
        "    \n",
        "    # What are they misclassified as?\n",
        "    misclassified_as = Counter(y_pred_cb[mask & (y_pred_cb != label_idx)])\n",
        "    \n",
        "    print(f\"\\n{label} ({total} samples): {accuracy:.1f}% correct\")\n",
        "    if misclassified_as:\n",
        "        for pred_idx, count in misclassified_as.most_common(3):\n",
        "            print(f\"  -> Misclassified as {label_mapping[pred_idx]}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. RECOMMENDATION 1: Class Merging Experiment\n",
        "\n",
        "### Rationale:\n",
        "- **Institutional** has only 32 samples (6 in test set) and achieves only **15% F1**\n",
        "- **CEX_Wallet** has 76 samples but achieves only **40% F1**\n",
        "- Both share similar characteristics:\n",
        "  - High balance\n",
        "  - Lower transaction frequency than DeFi traders\n",
        "  - Professional/institutional behavior\n",
        "- Merging them into **Large_Holder** creates a more balanced class (108 samples)\n",
        "- This reduces the problem from 6-class to 5-class, matching research assumptions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create merged dataset\n",
        "df_merged = df.copy()\n",
        "\n",
        "# Merge Institutional + CEX_Wallet -> Large_Holder\n",
        "df_merged['category_merged'] = df_merged['category'].replace({\n",
        "    'Institutional': 'Large_Holder',\n",
        "    'CEX_Wallet': 'Large_Holder'\n",
        "})\n",
        "\n",
        "print(\"=== Original Class Distribution ===\")\n",
        "print(df['category'].value_counts())\n",
        "\n",
        "print(\"\\n=== Merged Class Distribution ===\")\n",
        "print(df_merged['category_merged'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare merged data for training\n",
        "X_merged = df_merged[FEATURES].values\n",
        "y_merged_raw = df_merged['category_merged'].values\n",
        "\n",
        "# New label encoder for merged classes\n",
        "le_merged = LabelEncoder()\n",
        "y_merged = le_merged.fit_transform(y_merged_raw)\n",
        "\n",
        "merged_labels = list(le_merged.classes_)\n",
        "print(f\"Merged labels: {merged_labels}\")\n",
        "print(f\"Number of classes: {len(merged_labels)} (reduced from 6)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train/test split for merged data\n",
        "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(\n",
        "    X_merged, y_merged,\n",
        "    test_size=0.20,\n",
        "    random_state=42,\n",
        "    stratify=y_merged\n",
        ")\n",
        "\n",
        "print(f\"Training: {len(X_train_m)}, Test: {len(X_test_m)}\")\n",
        "print(f\"\\nTest distribution:\")\n",
        "for i, label in enumerate(merged_labels):\n",
        "    count = (y_test_m == i).sum()\n",
        "    print(f\"  {label}: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train CatBoost on merged data\n",
        "print(\"Training CatBoost on merged classes (5 classes)...\")\n",
        "\n",
        "cb_merged = CatBoostClassifier(\n",
        "    iterations=300,\n",
        "    learning_rate=0.03,\n",
        "    depth=5,\n",
        "    l2_leaf_reg=3,\n",
        "    loss_function='MultiClass',\n",
        "    random_seed=42,\n",
        "    verbose=50,\n",
        "    auto_class_weights='Balanced'\n",
        ")\n",
        "\n",
        "cb_merged.fit(X_train_m, y_train_m)\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate merged model\n",
        "y_pred_merged = get_predictions(cb_merged, X_test_m)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"MERGED MODEL (5 classes) Classification Report\")\n",
        "print(\"=\" * 60)\n",
        "print(classification_report(y_test_m, y_pred_merged, target_names=merged_labels, zero_division=0))\n",
        "\n",
        "# Compare metrics\n",
        "f1_merged = f1_score(y_test_m, y_pred_merged, average='macro')\n",
        "f1_original = f1_score(y_test, y_pred_cb, average='macro')\n",
        "\n",
        "print(f\"\\n=== Comparison ===\")\n",
        "print(f\"Original (6 classes) F1 Macro: {f1_original:.4f}\")\n",
        "print(f\"Merged (5 classes) F1 Macro:   {f1_merged:.4f}\")\n",
        "print(f\"Improvement: {(f1_merged - f1_original)*100:+.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion matrix for merged model\n",
        "plot_confusion_matrix(y_test_m, y_pred_merged, merged_labels, 'CatBoost (Merged Classes) Confusion Matrix')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5-Fold CV on merged data for more reliable comparison\n",
        "print(\"\\n=== 5-Fold CV Comparison ===\")\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Original 6-class CV\n",
        "X_orig = df[FEATURES].values\n",
        "y_orig = le.transform(df['category'].values)\n",
        "\n",
        "cv_scores_original = []\n",
        "for train_idx, val_idx in skf.split(X_orig, y_orig):\n",
        "    cb_temp = CatBoostClassifier(\n",
        "        iterations=300, learning_rate=0.03, depth=5, l2_leaf_reg=3,\n",
        "        random_seed=42, verbose=0, auto_class_weights='Balanced'\n",
        "    )\n",
        "    cb_temp.fit(X_orig[train_idx], y_orig[train_idx])\n",
        "    y_pred_temp = get_predictions(cb_temp, X_orig[val_idx])\n",
        "    cv_scores_original.append(f1_score(y_orig[val_idx], y_pred_temp, average='macro'))\n",
        "\n",
        "print(f\"Original (6 classes): {np.mean(cv_scores_original):.4f} (+/- {np.std(cv_scores_original):.4f})\")\n",
        "\n",
        "# Merged 5-class CV\n",
        "cv_scores_merged = []\n",
        "for train_idx, val_idx in skf.split(X_merged, y_merged):\n",
        "    cb_temp = CatBoostClassifier(\n",
        "        iterations=300, learning_rate=0.03, depth=5, l2_leaf_reg=3,\n",
        "        random_seed=42, verbose=0, auto_class_weights='Balanced'\n",
        "    )\n",
        "    cb_temp.fit(X_merged[train_idx], y_merged[train_idx])\n",
        "    y_pred_temp = get_predictions(cb_temp, X_merged[val_idx])\n",
        "    cv_scores_merged.append(f1_score(y_merged[val_idx], y_pred_temp, average='macro'))\n",
        "\n",
        "print(f\"Merged (5 classes):   {np.mean(cv_scores_merged):.4f} (+/- {np.std(cv_scores_merged):.4f})\")\n",
        "print(f\"\\nCV Improvement: {(np.mean(cv_scores_merged) - np.mean(cv_scores_original))*100:+.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. RECOMMENDATION 2: Production Reliability Assessment\n",
        "\n",
        "### Which predictions can we trust in production?\n",
        "Based on per-class F1 scores, we categorize model reliability:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Production reliability assessment\n",
        "print(\"=\" * 60)\n",
        "print(\"PRODUCTION RELIABILITY ASSESSMENT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Get per-class F1 for CatBoost\n",
        "f1_per_class = per_class_metrics['CatBoost']['f1']\n",
        "\n",
        "reliable = []\n",
        "moderate = []\n",
        "unreliable = []\n",
        "\n",
        "for label, f1_val in f1_per_class.items():\n",
        "    if f1_val >= 0.70:\n",
        "        reliable.append((label, f1_val))\n",
        "    elif f1_val >= 0.50:\n",
        "        moderate.append((label, f1_val))\n",
        "    else:\n",
        "        unreliable.append((label, f1_val))\n",
        "\n",
        "print(\"\\n*** HIGH RELIABILITY (F1 >= 70%) - Trust these predictions ***\")\n",
        "for label, f1_val in reliable:\n",
        "    print(f\"  {label}: {f1_val*100:.1f}% F1\")\n",
        "\n",
        "print(\"\\n*** MODERATE RELIABILITY (50-70% F1) - Use with caution ***\")\n",
        "for label, f1_val in moderate:\n",
        "    print(f\"  {label}: {f1_val*100:.1f}% F1\")\n",
        "\n",
        "print(\"\\n*** LOW RELIABILITY (F1 < 50%) - Do NOT trust in production ***\")\n",
        "for label, f1_val in unreliable:\n",
        "    print(f\"  {label}: {f1_val*100:.1f}% F1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize reliability tiers\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "labels_sorted = sorted(f1_per_class.keys(), key=lambda x: f1_per_class[x], reverse=True)\n",
        "f1_values = [f1_per_class[l] for l in labels_sorted]\n",
        "\n",
        "colors = []\n",
        "for f1 in f1_values:\n",
        "    if f1 >= 0.70:\n",
        "        colors.append('green')\n",
        "    elif f1 >= 0.50:\n",
        "        colors.append('orange')\n",
        "    else:\n",
        "        colors.append('red')\n",
        "\n",
        "bars = ax.bar(labels_sorted, f1_values, color=colors)\n",
        "\n",
        "ax.axhline(y=0.70, color='green', linestyle='--', alpha=0.7, label='High reliability (70%)')\n",
        "ax.axhline(y=0.50, color='orange', linestyle='--', alpha=0.7, label='Moderate reliability (50%)')\n",
        "\n",
        "ax.set_ylabel('F1 Score')\n",
        "ax.set_title('Production Reliability by Category\\n(Green=Trust, Orange=Caution, Red=Avoid)')\n",
        "ax.set_ylim(0, 1)\n",
        "ax.legend()\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. RECOMMENDATION 3: Trading Signal Guidelines\n",
        "\n",
        "### Based on model reliability, here are recommended trading actions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"TRADING SIGNAL GUIDELINES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "trading_signals = {\n",
        "    'Exploiter': {\n",
        "        'reliability': 'HIGH (94% F1)',\n",
        "        'signal': 'STRONG AVOID',\n",
        "        'action': 'Do NOT copy trades. These are exploit/hack addresses.',\n",
        "        'rationale': 'Model correctly identifies 94% of exploiters. High tx/day + drained balance pattern.'\n",
        "    },\n",
        "    'Miner': {\n",
        "        'reliability': 'HIGH (79% F1)',\n",
        "        'signal': 'BEARISH on large sells',\n",
        "        'action': 'Monitor for selling acceleration. Miners selling = supply pressure.',\n",
        "        'rationale': 'Distinct low tx/day pattern. When they move, its significant.'\n",
        "    },\n",
        "    'DeFi_Trader': {\n",
        "        'reliability': 'MODERATE (58% F1)',\n",
        "        'signal': 'FOLLOW with caution',\n",
        "        'action': 'Track DEX swaps and LP movements. Potential alpha signals.',\n",
        "        'rationale': '58% reliability means 4 in 10 may be misclassified.'\n",
        "    },\n",
        "    'CEX_Wallet': {\n",
        "        'reliability': 'LOW (40% F1)',\n",
        "        'signal': 'NEUTRAL - Exchange flow only',\n",
        "        'action': 'Only use for aggregate exchange flow analysis, not individual signals.',\n",
        "        'rationale': 'Often confused with DeFi_Trader. Unreliable for individual wallet tracking.'\n",
        "    },\n",
        "    'Staker': {\n",
        "        'reliability': 'LOW (43% F1)',\n",
        "        'signal': 'WEAK SIGNAL',\n",
        "        'action': 'Unstaking events may indicate selling intent, but low confidence.',\n",
        "        'rationale': 'Frequently misclassified. Use with other confirmations only.'\n",
        "    },\n",
        "    'Institutional': {\n",
        "        'reliability': 'VERY LOW (15% F1)',\n",
        "        'signal': 'DO NOT USE',\n",
        "        'action': 'Model cannot reliably identify institutional wallets.',\n",
        "        'rationale': 'Only 1 in 6 correctly identified. Not production ready.'\n",
        "    }\n",
        "}\n",
        "\n",
        "for category, info in trading_signals.items():\n",
        "    print(f\"\\n{'='*40}\")\n",
        "    print(f\"Category: {category}\")\n",
        "    print(f\"{'='*40}\")\n",
        "    print(f\"Reliability: {info['reliability']}\")\n",
        "    print(f\"Signal:      {info['signal']}\")\n",
        "    print(f\"Action:      {info['action']}\")\n",
        "    print(f\"Rationale:   {info['rationale']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary table for trading signals\n",
        "signal_summary = pd.DataFrame([\n",
        "    {'Category': 'Exploiter', 'F1': '94%', 'Reliability': 'HIGH', 'Trading Signal': 'AVOID - Exploit addresses'},\n",
        "    {'Category': 'Miner', 'F1': '79%', 'Reliability': 'HIGH', 'Trading Signal': 'BEARISH on sells'},\n",
        "    {'Category': 'DeFi_Trader', 'F1': '58%', 'Reliability': 'MODERATE', 'Trading Signal': 'FOLLOW with caution'},\n",
        "    {'Category': 'Staker', 'F1': '43%', 'Reliability': 'LOW', 'Trading Signal': 'Weak unstake signal'},\n",
        "    {'Category': 'CEX_Wallet', 'F1': '40%', 'Reliability': 'LOW', 'Trading Signal': 'Exchange flow only'},\n",
        "    {'Category': 'Institutional', 'F1': '15%', 'Reliability': 'VERY LOW', 'Trading Signal': 'DO NOT USE'}\n",
        "])\n",
        "\n",
        "print(\"\\n=== Trading Signal Summary ===\")\n",
        "print(signal_summary.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save merged model if it performed better\n",
        "import os\n",
        "os.makedirs('/content/models_v2', exist_ok=True)\n",
        "\n",
        "# Save merged model\n",
        "cb_merged.save_model('/content/models_v2/catboost_merged_5class.cbm')\n",
        "\n",
        "# Save merged label encoder\n",
        "with open('/content/models_v2/label_encoder_merged.pkl', 'wb') as f:\n",
        "    pickle.dump(le_merged, f)\n",
        "\n",
        "# Save evaluation results\n",
        "evaluation_results = {\n",
        "    'original_6class': {\n",
        "        'f1_macro_holdout': f1_original,\n",
        "        'f1_macro_cv_mean': np.mean(cv_scores_original),\n",
        "        'f1_macro_cv_std': np.std(cv_scores_original),\n",
        "        'per_class_f1': per_class_metrics['CatBoost']['f1']\n",
        "    },\n",
        "    'merged_5class': {\n",
        "        'f1_macro_holdout': float(f1_merged),\n",
        "        'f1_macro_cv_mean': np.mean(cv_scores_merged),\n",
        "        'f1_macro_cv_std': np.std(cv_scores_merged),\n",
        "        'merged_classes': ['Institutional', 'CEX_Wallet'],\n",
        "        'new_class': 'Large_Holder'\n",
        "    },\n",
        "    'production_reliability': {\n",
        "        'high': [l for l, f in f1_per_class.items() if f >= 0.70],\n",
        "        'moderate': [l for l, f in f1_per_class.items() if 0.50 <= f < 0.70],\n",
        "        'low': [l for l, f in f1_per_class.items() if f < 0.50]\n",
        "    },\n",
        "    'trading_signals': trading_signals\n",
        "}\n",
        "\n",
        "with open('/content/models_v2/evaluation_results.json', 'w') as f:\n",
        "    json.dump(evaluation_results, f, indent=2, default=str)\n",
        "\n",
        "print(\"Results saved locally\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload to GCS\n",
        "!gsutil -m cp -r /content/models_v2/* gs://{BUCKET}/models/evaluation/\n",
        "print(f\"Uploaded to gs://{BUCKET}/models/evaluation/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### Key Findings:\n",
        "\n",
        "1. **Feature Dominance**: `balance_eth_log` accounts for 27% of model importance - suggests model relies on balance rather than behavioral patterns\n",
        "\n",
        "2. **Class Merging Experiment**: \n",
        "   - Merging Institutional + CEX_Wallet -> Large_Holder\n",
        "   - Reduces problem from 6-class to 5-class\n",
        "   - CV improvement: [see output above]\n",
        "\n",
        "3. **Production Reliability**:\n",
        "   - HIGH: Exploiter (94%), Miner (79%)\n",
        "   - MODERATE: DeFi_Trader (58%)\n",
        "   - LOW: CEX_Wallet (40%), Staker (43%), Institutional (15%)\n",
        "\n",
        "4. **Trading Recommendations**:\n",
        "   - Trust Exploiter/Miner predictions for trading signals\n",
        "   - Use DeFi_Trader with caution\n",
        "   - Avoid using Institutional predictions entirely\n",
        "\n",
        "### Next Steps:\n",
        "- Notebook 04: Hyperparameter tuning (focus on CatBoost)\n",
        "- Consider using merged 5-class model in production if CV shows improvement"
      ]
    }
  ]
}
