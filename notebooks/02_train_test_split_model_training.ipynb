{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SMT-WEEX Notebook 2: Train/Test Split & Multi-Model Training\n",
        "**Project:** smt-weex-2025\n",
        "**Author:** Jannet Ekka\n",
        "\n",
        "This notebook:\n",
        "1. Loads cleaned data from GCS (output of notebook 01)\n",
        "2. Splits into train/val/test (70/10/20)\n",
        "3. Trains multiple models (CatBoost, XGBoost, RandomForest, LightGBM)\n",
        "4. Initial evaluation\n",
        "5. Saves models to GCS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install packages\n",
        "!pip install -q catboost xgboost lightgbm scikit-learn pandas numpy matplotlib seaborn google-cloud-storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Authenticate\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "PROJECT_ID = 'smt-weex-2025'\n",
        "BUCKET = 'smt-weex-2025-models'\n",
        "\n",
        "!gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    classification_report, confusion_matrix\n",
        ")\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Cleaned Data from GCS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download from GCS (output of notebook 01)\n",
        "!gsutil cp gs://{BUCKET}/data/whale_features_cleaned.csv /content/\n",
        "!gsutil cp gs://{BUCKET}/data/feature_config.json /content/\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('/content/whale_features_cleaned.csv')\n",
        "\n",
        "# Load feature config\n",
        "with open('/content/feature_config.json', 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "FEATURES = config['features']\n",
        "TARGET = config['target']\n",
        "\n",
        "print(f\"Loaded {len(df)} samples\")\n",
        "print(f\"Features: {len(FEATURES)}\")\n",
        "print(f\"Categories: {df[TARGET].value_counts().to_dict()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare X and y\n",
        "X = df[FEATURES].values\n",
        "y_raw = df[TARGET].values\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y_raw)\n",
        "\n",
        "# Save label mapping\n",
        "label_mapping = {i: label for i, label in enumerate(le.classes_)}\n",
        "print(\"Label mapping:\")\n",
        "for idx, label in label_mapping.items():\n",
        "    count = (y == idx).sum()\n",
        "    print(f\"  {idx}: {label} ({count} samples)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Train/Val/Test Split (70/10/20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First split: 80% train+val, 20% test\n",
        "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
        "    X, y, \n",
        "    test_size=0.20, \n",
        "    random_state=42, \n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "# Second split: 87.5% train, 12.5% val (from trainval) = 70/10 overall\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_trainval, y_trainval,\n",
        "    test_size=0.125,  # 0.125 * 0.8 = 0.1 (10% of total)\n",
        "    random_state=42,\n",
        "    stratify=y_trainval\n",
        ")\n",
        "\n",
        "print(f\"Train: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
        "print(f\"Val:   {len(X_val)} ({len(X_val)/len(X)*100:.1f}%)\")\n",
        "print(f\"Test:  {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check stratification\n",
        "print(\"\\n=== Class Distribution ===\")\n",
        "for name, y_subset in [('Train', y_train), ('Val', y_val), ('Test', y_test)]:\n",
        "    unique, counts = np.unique(y_subset, return_counts=True)\n",
        "    print(f\"{name}: {dict(zip([label_mapping[u] for u in unique], counts))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Store models and results\n",
        "models = {}\n",
        "results = {}\n",
        "n_classes = len(label_mapping)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 CatBoost (Primary Model - Best for Small Data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 50)\n",
        "print(\"Training CatBoost...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "catboost_model = CatBoostClassifier(\n",
        "    iterations=500,\n",
        "    learning_rate=0.03,\n",
        "    depth=4,\n",
        "    l2_leaf_reg=3,\n",
        "    loss_function='MultiClass',\n",
        "    eval_metric='Accuracy',\n",
        "    random_seed=42,\n",
        "    verbose=100,\n",
        "    early_stopping_rounds=50,\n",
        "    auto_class_weights='Balanced'  # Handle class imbalance\n",
        ")\n",
        "\n",
        "catboost_model.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=(X_val, y_val),\n",
        "    use_best_model=True\n",
        ")\n",
        "\n",
        "models['CatBoost'] = catboost_model\n",
        "print(\"CatBoost training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 50)\n",
        "print(\"Training XGBoost...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Calculate class weights for XGBoost\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "sample_weights = compute_sample_weight('balanced', y_train)\n",
        "\n",
        "xgb_model = XGBClassifier(\n",
        "    n_estimators=500,\n",
        "    learning_rate=0.03,\n",
        "    max_depth=4,\n",
        "    reg_alpha=1,\n",
        "    reg_lambda=3,\n",
        "    objective='multi:softmax',\n",
        "    num_class=n_classes,\n",
        "    random_state=42,\n",
        "    early_stopping_rounds=50,\n",
        "    eval_metric='mlogloss'\n",
        ")\n",
        "\n",
        "xgb_model.fit(\n",
        "    X_train, y_train,\n",
        "    sample_weight=sample_weights,\n",
        "    eval_set=[(X_val, y_val)],\n",
        "    verbose=100\n",
        ")\n",
        "\n",
        "models['XGBoost'] = xgb_model\n",
        "print(\"XGBoost training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 50)\n",
        "print(\"Training Random Forest...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=500,\n",
        "    max_depth=10,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=3,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    class_weight='balanced'\n",
        ")\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "models['RandomForest'] = rf_model\n",
        "print(\"Random Forest training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4 LightGBM (Not Recommended for Small Data - Testing Only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 50)\n",
        "print(\"Training LightGBM (WARNING: Not ideal for <10K samples)...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "lgbm_model = LGBMClassifier(\n",
        "    n_estimators=500,\n",
        "    learning_rate=0.03,\n",
        "    max_depth=4,\n",
        "    num_leaves=15,\n",
        "    reg_alpha=1,\n",
        "    reg_lambda=3,\n",
        "    objective='multiclass',\n",
        "    num_class=n_classes,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    class_weight='balanced',\n",
        "    verbosity=-1\n",
        ")\n",
        "\n",
        "lgbm_model.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_val, y_val)]\n",
        ")\n",
        "\n",
        "models['LightGBM'] = lgbm_model\n",
        "print(\"LightGBM training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Initial Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, X_test, y_test, model_name):\n",
        "    \"\"\"Evaluate a model and return metrics\"\"\"\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    metrics = {\n",
        "        'accuracy': accuracy_score(y_test, y_pred),\n",
        "        'precision_macro': precision_score(y_test, y_pred, average='macro', zero_division=0),\n",
        "        'recall_macro': recall_score(y_test, y_pred, average='macro', zero_division=0),\n",
        "        'f1_macro': f1_score(y_test, y_pred, average='macro', zero_division=0),\n",
        "        'f1_weighted': f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    }\n",
        "    \n",
        "    return metrics, y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate all models on test set\n",
        "print(\"=\" * 60)\n",
        "print(\"MODEL EVALUATION ON TEST SET\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for name, model in models.items():\n",
        "    metrics, y_pred = evaluate_model(model, X_test, y_test, name)\n",
        "    results[name] = metrics\n",
        "    \n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Accuracy:         {metrics['accuracy']:.4f}\")\n",
        "    print(f\"  Precision (macro): {metrics['precision_macro']:.4f}\")\n",
        "    print(f\"  Recall (macro):    {metrics['recall_macro']:.4f}\")\n",
        "    print(f\"  F1 (macro):        {metrics['f1_macro']:.4f}\")\n",
        "    print(f\"  F1 (weighted):     {metrics['f1_weighted']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Results comparison table\n",
        "results_df = pd.DataFrame(results).T\n",
        "results_df = results_df.round(4)\n",
        "print(\"\\n=== Model Comparison ===\")\n",
        "print(results_df.sort_values('f1_macro', ascending=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize comparison\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "metrics_to_plot = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
        "x = np.arange(len(models))\n",
        "width = 0.2\n",
        "\n",
        "for i, metric in enumerate(metrics_to_plot):\n",
        "    values = [results[m][metric] for m in models.keys()]\n",
        "    ax.bar(x + i*width, values, width, label=metric)\n",
        "\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('Model Performance Comparison')\n",
        "ax.set_xticks(x + width * 1.5)\n",
        "ax.set_xticklabels(models.keys())\n",
        "ax.legend()\n",
        "ax.set_ylim(0, 1)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find best model\n",
        "best_model_name = max(results, key=lambda x: results[x]['f1_macro'])\n",
        "print(f\"\\nBest Model: {best_model_name} (F1 macro: {results[best_model_name]['f1_macro']:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Save Models to GCS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save all models\n",
        "import os\n",
        "os.makedirs('/content/models', exist_ok=True)\n",
        "\n",
        "# CatBoost native format\n",
        "catboost_model.save_model('/content/models/catboost_whale_classifier.cbm')\n",
        "\n",
        "# Others as pickle\n",
        "with open('/content/models/xgboost_whale_classifier.pkl', 'wb') as f:\n",
        "    pickle.dump(xgb_model, f)\n",
        "\n",
        "with open('/content/models/randomforest_whale_classifier.pkl', 'wb') as f:\n",
        "    pickle.dump(rf_model, f)\n",
        "\n",
        "with open('/content/models/lightgbm_whale_classifier.pkl', 'wb') as f:\n",
        "    pickle.dump(lgbm_model, f)\n",
        "\n",
        "# Save label encoder\n",
        "with open('/content/models/label_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(le, f)\n",
        "\n",
        "# Save results\n",
        "with open('/content/models/initial_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"Models saved locally\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload to GCS\n",
        "!gsutil -m cp -r /content/models/* gs://{BUCKET}/models/initial/\n",
        "print(f\"Models uploaded to gs://{BUCKET}/models/initial/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save train/val/test splits for reproducibility\n",
        "np.savez('/content/data_splits.npz',\n",
        "         X_train=X_train, y_train=y_train,\n",
        "         X_val=X_val, y_val=y_val,\n",
        "         X_test=X_test, y_test=y_test)\n",
        "\n",
        "!gsutil cp /content/data_splits.npz gs://{BUCKET}/data/data_splits.npz\n",
        "print(\"Data splits saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Training completed:\n",
        "1. Split data: 70% train, 10% val, 20% test\n",
        "2. Trained 4 models: CatBoost, XGBoost, RandomForest, LightGBM\n",
        "3. Initial evaluation on test set\n",
        "4. Saved all models to GCS\n",
        "\n",
        "**Best Model:** [See output above]\n",
        "\n",
        "**Next:** Run Notebook 3 for detailed evaluation and insights."
      ]
    }
  ]
}
