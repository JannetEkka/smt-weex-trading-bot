{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SMT-WEEX Notebook 1: Data Cleaning & Feature Engineering\n",
        "**Project:** smt-weex-2025\n",
        "**Author:** Jannet Ekka\n",
        "\n",
        "This notebook:\n",
        "1. Loads data from BigQuery\n",
        "2. Cleans and validates features\n",
        "3. Handles missing values and outliers\n",
        "4. Feature transformations\n",
        "5. Saves cleaned dataset to GCS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Authentication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q google-cloud-bigquery google-cloud-storage pandas numpy scikit-learn catboost db-dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Authenticate with Google Cloud\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Set project\n",
        "PROJECT_ID = 'smt-weex-2025'\n",
        "BUCKET = 'smt-weex-2025-models'\n",
        "\n",
        "!gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import storage\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Initialize BigQuery client\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "print(f\"Connected to project: {PROJECT_ID}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Data from BigQuery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load whale features from BigQuery\n",
        "query = \"\"\"\n",
        "SELECT * FROM `smt-weex-2025.ml_data.whale_features`\n",
        "\"\"\"\n",
        "\n",
        "df = bq_client.query(query).to_dataframe()\n",
        "print(f\"Loaded {len(df)} rows, {len(df.columns)} columns from BigQuery\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check columns and dtypes\n",
        "print(\"=== Column Info ===\")\n",
        "print(df.dtypes)\n",
        "print(f\"\\n=== Shape: {df.shape} ===\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check category distribution\n",
        "print(\"=== Category Distribution ===\")\n",
        "print(df['category'].value_counts())\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(10, 5))\n",
        "df['category'].value_counts().plot(kind='bar', color='steelblue')\n",
        "plt.title('Whale Category Distribution')\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"=== Missing Values ===\")\n",
        "missing = df.isnull().sum()\n",
        "missing = missing[missing > 0]\n",
        "if len(missing) > 0:\n",
        "    print(missing)\n",
        "else:\n",
        "    print(\"No missing values!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for duplicates\n",
        "duplicates = df['address'].duplicated().sum()\n",
        "print(f\"Duplicate addresses: {duplicates}\")\n",
        "\n",
        "# Remove duplicates if any\n",
        "if duplicates > 0:\n",
        "    df = df.drop_duplicates(subset=['address'], keep='first')\n",
        "    print(f\"After dedup: {len(df)} rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define feature columns (exclude non-feature columns)\n",
        "NON_FEATURES = ['address', 'category', 'sub_label']\n",
        "FEATURE_COLS = [col for col in df.columns if col not in NON_FEATURES]\n",
        "\n",
        "print(f\"Feature columns ({len(FEATURE_COLS)}):\")\n",
        "print(FEATURE_COLS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for infinite values\n",
        "print(\"=== Infinite Values ===\")\n",
        "for col in FEATURE_COLS:\n",
        "    if df[col].dtype in ['float64', 'int64', 'Float64', 'Int64']:\n",
        "        inf_count = np.isinf(df[col].astype(float)).sum()\n",
        "        if inf_count > 0:\n",
        "            print(f\"{col}: {inf_count} infinite values\")\n",
        "            # Replace inf with max non-inf value\n",
        "            max_val = df[col][~np.isinf(df[col].astype(float))].max()\n",
        "            df[col] = df[col].replace([np.inf, -np.inf], max_val)\n",
        "            print(f\"  -> Replaced with {max_val}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic statistics\n",
        "print(\"=== Feature Statistics ===\")\n",
        "df[FEATURE_COLS].describe().T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Outlier Detection & Handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for extreme outliers using IQR\n",
        "def detect_outliers_iqr(df, col):\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower = Q1 - 3 * IQR  # Using 3x IQR for extreme outliers\n",
        "    upper = Q3 + 3 * IQR\n",
        "    outliers = df[(df[col] < lower) | (df[col] > upper)]\n",
        "    return len(outliers), lower, upper\n",
        "\n",
        "print(\"=== Outlier Detection (3x IQR) ===\")\n",
        "for col in FEATURE_COLS:\n",
        "    if df[col].dtype in ['float64', 'int64', 'Float64', 'Int64']:\n",
        "        count, lower, upper = detect_outliers_iqr(df, col)\n",
        "        if count > 0:\n",
        "            pct = count / len(df) * 100\n",
        "            print(f\"{col}: {count} outliers ({pct:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We'll use log transformation for highly skewed features instead of removing outliers\n",
        "# This preserves whale behavior patterns\n",
        "\n",
        "SKEWED_COLS = [\n",
        "    'total_txs', 'outgoing_count', 'incoming_count',\n",
        "    'outgoing_volume_eth', 'incoming_volume_eth',\n",
        "    'avg_tx_value_eth', 'max_tx_value_eth', 'std_tx_value_eth',\n",
        "    'avg_gas_used', 'max_gas_used',\n",
        "    'unique_counterparties', 'unique_tokens', 'balance_eth'\n",
        "]\n",
        "\n",
        "# Create log-transformed versions\n",
        "for col in SKEWED_COLS:\n",
        "    if col in df.columns:\n",
        "        df[f'{col}_log'] = np.log1p(df[col].clip(lower=0).astype(float))\n",
        "        print(f\"Created {col}_log\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize distribution before/after log transform\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
        "\n",
        "sample_cols = ['total_txs', 'balance_eth', 'unique_counterparties']\n",
        "for i, col in enumerate(sample_cols):\n",
        "    if col in df.columns:\n",
        "        # Original\n",
        "        axes[0, i].hist(df[col].astype(float), bins=50, color='steelblue', alpha=0.7)\n",
        "        axes[0, i].set_title(f'{col} (original)')\n",
        "        \n",
        "        # Log transformed\n",
        "        axes[1, i].hist(df[f'{col}_log'], bins=50, color='coral', alpha=0.7)\n",
        "        axes[1, i].set_title(f'{col} (log)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create additional derived features\n",
        "\n",
        "# 1. Activity intensity score\n",
        "df['activity_intensity'] = df['total_txs'].astype(float) / (df['activity_span_days'].astype(float) + 1)\n",
        "\n",
        "# 2. DeFi engagement score\n",
        "df['defi_engagement'] = (df['defi_interactions'].astype(float) + df['unique_defi_protocols'].astype(float)) / (df['total_txs'].astype(float) + 1)\n",
        "\n",
        "# 3. Token diversity normalized\n",
        "df['token_diversity_norm'] = df['unique_tokens'].astype(float) / (df['erc20_tx_count'].astype(float) + 1)\n",
        "\n",
        "# 4. Value concentration (max/avg ratio)\n",
        "df['value_concentration'] = df['max_tx_value_eth'].astype(float) / (df['avg_tx_value_eth'].astype(float) + 0.001)\n",
        "\n",
        "# 5. Flow imbalance (absolute)\n",
        "df['flow_imbalance'] = abs(df['net_flow_eth'].astype(float)) / (df['incoming_volume_eth'].astype(float) + df['outgoing_volume_eth'].astype(float) + 0.001)\n",
        "\n",
        "# 6. Gas efficiency (inverse of avg gas)\n",
        "df['gas_efficiency'] = 1 / (df['avg_gas_used'].astype(float) + 1)\n",
        "\n",
        "print(\"Created 6 new derived features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Handle net_flow_eth which can be extremely large positive or negative\n",
        "# Use signed log transformation\n",
        "def signed_log(x):\n",
        "    return np.sign(x) * np.log1p(abs(x))\n",
        "\n",
        "df['net_flow_eth_signed_log'] = df['net_flow_eth'].astype(float).apply(signed_log)\n",
        "print(\"Created net_flow_eth_signed_log\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final feature list for ML\n",
        "ML_FEATURES = [\n",
        "    # Original features (non-skewed)\n",
        "    'erc20_ratio', 'nft_ratio', 'internal_ratio',\n",
        "    'large_tx_ratio', 'avg_time_between_tx_hours', 'std_time_between_tx_hours',\n",
        "    'tx_per_day', 'business_hour_ratio', 'peak_hour_pct',\n",
        "    'defi_interactions', 'unique_defi_protocols', 'cex_interactions',\n",
        "    'stablecoin_ratio', 'tx_ratio_out_in',\n",
        "    \n",
        "    # Log-transformed features\n",
        "    'total_txs_log', 'outgoing_count_log', 'incoming_count_log',\n",
        "    'outgoing_volume_eth_log', 'incoming_volume_eth_log',\n",
        "    'avg_tx_value_eth_log', 'max_tx_value_eth_log',\n",
        "    'avg_gas_used_log', 'max_gas_used_log',\n",
        "    'unique_counterparties_log', 'unique_tokens_log', 'balance_eth_log',\n",
        "    \n",
        "    # Derived features\n",
        "    'activity_intensity', 'defi_engagement', 'token_diversity_norm',\n",
        "    'value_concentration', 'flow_imbalance', 'gas_efficiency',\n",
        "    'net_flow_eth_signed_log'\n",
        "]\n",
        "\n",
        "# Check which features exist\n",
        "ML_FEATURES = [f for f in ML_FEATURES if f in df.columns]\n",
        "print(f\"Final ML features ({len(ML_FEATURES)}):\")\n",
        "print(ML_FEATURES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Feature Correlation Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation matrix\n",
        "plt.figure(figsize=(16, 14))\n",
        "corr_matrix = df[ML_FEATURES].corr()\n",
        "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0)\n",
        "plt.title('Feature Correlation Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find highly correlated features (>0.9)\n",
        "print(\"=== Highly Correlated Features (>0.9) ===\")\n",
        "high_corr_pairs = []\n",
        "for i in range(len(corr_matrix.columns)):\n",
        "    for j in range(i+1, len(corr_matrix.columns)):\n",
        "        if abs(corr_matrix.iloc[i, j]) > 0.9:\n",
        "            high_corr_pairs.append((\n",
        "                corr_matrix.columns[i],\n",
        "                corr_matrix.columns[j],\n",
        "                corr_matrix.iloc[i, j]\n",
        "            ))\n",
        "\n",
        "for f1, f2, corr in high_corr_pairs:\n",
        "    print(f\"{f1} <-> {f2}: {corr:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove highly correlated features (keep one from each pair)\n",
        "# Based on correlation analysis, remove features that are redundant\n",
        "FEATURES_TO_DROP = []\n",
        "\n",
        "# Add features to drop based on correlation analysis above\n",
        "# For example, if outgoing_count_log and incoming_count_log are highly correlated with total_txs_log\n",
        "# we might drop them\n",
        "\n",
        "FINAL_FEATURES = [f for f in ML_FEATURES if f not in FEATURES_TO_DROP]\n",
        "print(f\"Final features after removing correlations: {len(FINAL_FEATURES)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save Cleaned Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare final dataset\n",
        "df_clean = df[['address', 'category', 'sub_label'] + FINAL_FEATURES].copy()\n",
        "\n",
        "# Fill any remaining NaN with 0\n",
        "df_clean = df_clean.fillna(0)\n",
        "\n",
        "# Replace any remaining inf\n",
        "df_clean = df_clean.replace([np.inf, -np.inf], 0)\n",
        "\n",
        "print(f\"Final dataset shape: {df_clean.shape}\")\n",
        "print(f\"Categories: {df_clean['category'].value_counts().to_dict()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create GCS bucket if not exists\n",
        "!gsutil ls gs://{BUCKET} || gsutil mb -l us-central1 gs://{BUCKET}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save locally\n",
        "df_clean.to_csv('/content/whale_features_cleaned.csv', index=False)\n",
        "print(\"Saved to /content/whale_features_cleaned.csv\")\n",
        "\n",
        "# Upload to GCS\n",
        "!gsutil cp /content/whale_features_cleaned.csv gs://{BUCKET}/data/whale_features_cleaned.csv\n",
        "print(f\"Uploaded to gs://{BUCKET}/data/whale_features_cleaned.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Also save feature list for reference\n",
        "import json\n",
        "\n",
        "feature_config = {\n",
        "    'features': FINAL_FEATURES,\n",
        "    'target': 'category',\n",
        "    'categories': df_clean['category'].unique().tolist(),\n",
        "    'n_samples': len(df_clean),\n",
        "    'n_features': len(FINAL_FEATURES)\n",
        "}\n",
        "\n",
        "with open('/content/feature_config.json', 'w') as f:\n",
        "    json.dump(feature_config, f, indent=2)\n",
        "\n",
        "!gsutil cp /content/feature_config.json gs://{BUCKET}/data/feature_config.json\n",
        "print(\"Saved feature config\")\n",
        "print(json.dumps(feature_config, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Data cleaning completed:\n",
        "1. Loaded 516 whales from BigQuery\n",
        "2. Removed duplicates and handled missing values\n",
        "3. Applied log transformations to skewed features\n",
        "4. Created 7 derived features\n",
        "5. Analyzed correlations\n",
        "6. Saved cleaned dataset to GCS\n",
        "\n",
        "**Next:** Run Notebook 2 for train/test split and model training."
      ]
    }
  ]
}
