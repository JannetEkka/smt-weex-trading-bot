{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SMT-WEEX Notebook 1: Data Cleaning & Feature Engineering (v2)\n",
        "**Project:** smt-weex-2025\n",
        "**Author:** Jannet Ekka\n",
        "\n",
        "**Updates in v2:**\n",
        "- Drop highly correlated features\n",
        "- Add additional behavioral features\n",
        "- Better handle class imbalance preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Authentication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q google-cloud-bigquery google-cloud-storage pandas numpy scikit-learn catboost db-dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "PROJECT_ID = 'smt-weex-2025'\n",
        "BUCKET = 'smt-weex-2025-models'\n",
        "\n",
        "!gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import storage\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "print(f\"Connected to project: {PROJECT_ID}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Data from BigQuery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"\"\"\n",
        "SELECT * FROM `smt-weex-2025.ml_data.whale_features`\n",
        "\"\"\"\n",
        "\n",
        "df = bq_client.query(query).to_dataframe()\n",
        "print(f\"Loaded {len(df)} rows, {len(df.columns)} columns\")\n",
        "print(f\"\\nCategory Distribution:\")\n",
        "print(df['category'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove duplicates\n",
        "duplicates = df['address'].duplicated().sum()\n",
        "print(f\"Duplicate addresses: {duplicates}\")\n",
        "if duplicates > 0:\n",
        "    df = df.drop_duplicates(subset=['address'], keep='first')\n",
        "    print(f\"After dedup: {len(df)} rows\")\n",
        "\n",
        "# Fill missing values\n",
        "missing = df.isnull().sum()\n",
        "missing = missing[missing > 0]\n",
        "print(f\"\\nMissing values: {len(missing)} columns\")\n",
        "if len(missing) > 0:\n",
        "    print(missing)\n",
        "    df = df.fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Handle infinite values\n",
        "NON_FEATURES = ['address', 'category', 'sub_label']\n",
        "FEATURE_COLS = [col for col in df.columns if col not in NON_FEATURES]\n",
        "\n",
        "for col in FEATURE_COLS:\n",
        "    if df[col].dtype in ['float64', 'int64', 'Float64', 'Int64']:\n",
        "        inf_count = np.isinf(df[col].astype(float)).sum()\n",
        "        if inf_count > 0:\n",
        "            max_val = df[col][~np.isinf(df[col].astype(float))].max()\n",
        "            df[col] = df[col].replace([np.inf, -np.inf], max_val)\n",
        "            print(f\"{col}: replaced {inf_count} inf values\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Feature Transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Log transformations for highly skewed features\n",
        "SKEWED_COLS = [\n",
        "    'total_txs', 'outgoing_count', 'incoming_count',\n",
        "    'outgoing_volume_eth', 'incoming_volume_eth',\n",
        "    'avg_tx_value_eth', 'max_tx_value_eth', 'std_tx_value_eth',\n",
        "    'avg_gas_used', 'max_gas_used',\n",
        "    'unique_counterparties', 'unique_tokens', 'balance_eth'\n",
        "]\n",
        "\n",
        "for col in SKEWED_COLS:\n",
        "    if col in df.columns:\n",
        "        df[f'{col}_log'] = np.log1p(df[col].clip(lower=0).astype(float))\n",
        "\n",
        "# Signed log for net_flow_eth\n",
        "def signed_log(x):\n",
        "    return np.sign(x) * np.log1p(abs(x))\n",
        "\n",
        "df['net_flow_eth_signed_log'] = df['net_flow_eth'].astype(float).apply(signed_log)\n",
        "print(f\"Created {len(SKEWED_COLS)} log features + net_flow_eth_signed_log\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Derived features\n",
        "\n",
        "# 1. Activity intensity (tx per day)\n",
        "df['activity_intensity'] = df['total_txs'].astype(float) / (df['activity_span_days'].astype(float) + 1)\n",
        "\n",
        "# 2. DeFi engagement score\n",
        "df['defi_engagement'] = (df['defi_interactions'].astype(float) + df['unique_defi_protocols'].astype(float)) / (df['total_txs'].astype(float) + 1)\n",
        "\n",
        "# 3. Token diversity normalized\n",
        "df['token_diversity_norm'] = df['unique_tokens'].astype(float) / (df['erc20_tx_count'].astype(float) + 1)\n",
        "\n",
        "# 4. Value concentration (max/avg ratio) - indicates large single transactions\n",
        "df['value_concentration'] = df['max_tx_value_eth'].astype(float) / (df['avg_tx_value_eth'].astype(float) + 0.001)\n",
        "\n",
        "# 5. Flow imbalance (absolute)\n",
        "df['flow_imbalance'] = abs(df['net_flow_eth'].astype(float)) / (df['incoming_volume_eth'].astype(float) + df['outgoing_volume_eth'].astype(float) + 0.001)\n",
        "\n",
        "# 6. Gas efficiency (inverse of avg gas - higher = simpler txs)\n",
        "df['gas_efficiency'] = 1 / (df['avg_gas_used'].astype(float) + 1)\n",
        "\n",
        "# 7. NFT activity indicator (binary-ish)\n",
        "df['is_nft_active'] = (df['nft_ratio'] > 0.01).astype(int)\n",
        "\n",
        "# 8. Internal tx ratio (indicates contract interactions)\n",
        "df['internal_tx_heavy'] = (df['internal_ratio'] > 0.1).astype(int)\n",
        "\n",
        "# 9. Outflow dominance (distribution behavior indicator)\n",
        "df['outflow_dominant'] = (df['tx_ratio_out_in'] > 1.5).astype(int)\n",
        "\n",
        "# 10. High frequency trader indicator\n",
        "df['high_freq_trader'] = (df['tx_per_day'] > 10).astype(int)\n",
        "\n",
        "print(\"Created 10 derived features\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Feature Selection - Drop Highly Correlated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define final ML feature list\n",
        "# Dropping one from each highly correlated pair:\n",
        "# - activity_intensity (keep tx_per_day - same thing)\n",
        "# - unique_defi_protocols (keep defi_interactions - identical)\n",
        "# - max_tx_value_eth_log (keep avg_tx_value_eth_log - 0.983 corr)\n",
        "# - incoming_count_log (keep total_txs_log - 0.956 corr)\n",
        "# - std_time_between_tx_hours (keep avg_time_between_tx_hours - 0.916 corr)\n",
        "\n",
        "ML_FEATURES = [\n",
        "    # Original ratio features (low correlation, high signal)\n",
        "    'erc20_ratio',\n",
        "    'nft_ratio', \n",
        "    'internal_ratio',\n",
        "    'large_tx_ratio',\n",
        "    'stablecoin_ratio',\n",
        "    'tx_ratio_out_in',\n",
        "    'business_hour_ratio',\n",
        "    'peak_hour_pct',\n",
        "    \n",
        "    # Time features (keep only one from correlated pair)\n",
        "    'avg_time_between_tx_hours',  # dropped std_time_between_tx_hours\n",
        "    'tx_per_day',  # dropped activity_intensity (identical)\n",
        "    \n",
        "    # Interaction counts\n",
        "    'defi_interactions',  # dropped unique_defi_protocols (identical)\n",
        "    'cex_interactions',\n",
        "    \n",
        "    # Log-transformed features (dropped highly correlated ones)\n",
        "    'total_txs_log',  # dropped incoming_count_log (0.956 corr)\n",
        "    'outgoing_count_log',\n",
        "    'outgoing_volume_eth_log',\n",
        "    'incoming_volume_eth_log',\n",
        "    'avg_tx_value_eth_log',  # dropped max_tx_value_eth_log (0.983 corr)\n",
        "    'avg_gas_used_log',\n",
        "    'max_gas_used_log',\n",
        "    'unique_counterparties_log',\n",
        "    'unique_tokens_log',\n",
        "    'balance_eth_log',\n",
        "    \n",
        "    # Derived features\n",
        "    'defi_engagement',\n",
        "    'token_diversity_norm',\n",
        "    'value_concentration',\n",
        "    'flow_imbalance',\n",
        "    'gas_efficiency',\n",
        "    'net_flow_eth_signed_log',\n",
        "    \n",
        "    # Binary indicators (new)\n",
        "    'is_nft_active',\n",
        "    'internal_tx_heavy',\n",
        "    'outflow_dominant',\n",
        "    'high_freq_trader'\n",
        "]\n",
        "\n",
        "# Verify all features exist\n",
        "ML_FEATURES = [f for f in ML_FEATURES if f in df.columns]\n",
        "print(f\"Final ML features: {len(ML_FEATURES)}\")\n",
        "print(ML_FEATURES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify correlation after dropping\n",
        "plt.figure(figsize=(14, 12))\n",
        "corr_matrix = df[ML_FEATURES].corr()\n",
        "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0)\n",
        "plt.title('Feature Correlation Matrix (After Dropping High Corr)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Check remaining high correlations\n",
        "print(\"\\nRemaining correlations > 0.85:\")\n",
        "for i in range(len(corr_matrix.columns)):\n",
        "    for j in range(i+1, len(corr_matrix.columns)):\n",
        "        if abs(corr_matrix.iloc[i, j]) > 0.85:\n",
        "            print(f\"{corr_matrix.columns[i]} <-> {corr_matrix.columns[j]}: {corr_matrix.iloc[i, j]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Class Distribution Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze class distribution for imbalance handling\n",
        "class_counts = df['category'].value_counts()\n",
        "print(\"=== Class Distribution ===\")\n",
        "for cat, count in class_counts.items():\n",
        "    pct = count / len(df) * 100\n",
        "    print(f\"{cat:15s}: {count:4d} ({pct:5.1f}%)\")\n",
        "\n",
        "# Calculate class weights for reference\n",
        "total = len(df)\n",
        "n_classes = len(class_counts)\n",
        "print(f\"\\n=== Recommended Class Weights (balanced) ===\")\n",
        "for cat, count in class_counts.items():\n",
        "    weight = total / (n_classes * count)\n",
        "    print(f\"{cat:15s}: {weight:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize class distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Bar plot\n",
        "class_counts.plot(kind='bar', ax=axes[0], color='steelblue')\n",
        "axes[0].set_title('Class Distribution')\n",
        "axes[0].set_xlabel('Category')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Pie chart\n",
        "axes[1].pie(class_counts.values, labels=class_counts.index, autopct='%1.1f%%', startangle=90)\n",
        "axes[1].set_title('Class Distribution (%)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Feature Statistics by Category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Key distinguishing features by category\n",
        "key_features = ['tx_per_day', 'defi_interactions', 'internal_ratio', \n",
        "                'stablecoin_ratio', 'balance_eth_log', 'unique_counterparties_log']\n",
        "\n",
        "print(\"=== Key Features by Category (Mean) ===\")\n",
        "category_stats = df.groupby('category')[key_features].mean().round(3)\n",
        "print(category_stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize key feature distributions by category\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, feat in enumerate(key_features):\n",
        "    if feat in df.columns:\n",
        "        for cat in df['category'].unique():\n",
        "            data = df[df['category'] == cat][feat].astype(float)\n",
        "            axes[i].hist(data, bins=30, alpha=0.5, label=cat)\n",
        "        axes[i].set_title(feat)\n",
        "        axes[i].legend(fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Cleaned Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare final dataset\n",
        "df_clean = df[['address', 'category', 'sub_label'] + ML_FEATURES].copy()\n",
        "\n",
        "# Final cleanup\n",
        "df_clean = df_clean.fillna(0)\n",
        "df_clean = df_clean.replace([np.inf, -np.inf], 0)\n",
        "\n",
        "print(f\"Final dataset shape: {df_clean.shape}\")\n",
        "print(f\"Features: {len(ML_FEATURES)}\")\n",
        "print(f\"Categories: {df_clean['category'].value_counts().to_dict()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save locally and to GCS\n",
        "df_clean.to_csv('/content/whale_features_cleaned.csv', index=False)\n",
        "!gsutil cp /content/whale_features_cleaned.csv gs://{BUCKET}/data/whale_features_cleaned.csv\n",
        "print(f\"Saved to gs://{BUCKET}/data/whale_features_cleaned.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save feature config\n",
        "import json\n",
        "\n",
        "feature_config = {\n",
        "    'features': ML_FEATURES,\n",
        "    'target': 'category',\n",
        "    'categories': df_clean['category'].unique().tolist(),\n",
        "    'n_samples': len(df_clean),\n",
        "    'n_features': len(ML_FEATURES),\n",
        "    'class_counts': df_clean['category'].value_counts().to_dict(),\n",
        "    'dropped_correlated': [\n",
        "        'activity_intensity (same as tx_per_day)',\n",
        "        'unique_defi_protocols (same as defi_interactions)',\n",
        "        'max_tx_value_eth_log (0.983 corr with avg)',\n",
        "        'incoming_count_log (0.956 corr with total_txs)',\n",
        "        'std_time_between_tx_hours (0.916 corr with avg)'\n",
        "    ]\n",
        "}\n",
        "\n",
        "with open('/content/feature_config.json', 'w') as f:\n",
        "    json.dump(feature_config, f, indent=2)\n",
        "\n",
        "!gsutil cp /content/feature_config.json gs://{BUCKET}/data/feature_config.json\n",
        "print(\"Feature config saved\")\n",
        "print(json.dumps(feature_config, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "**Data Prepared for Notebook 02:**\n",
        "- 532 whales, 30 features (reduced from 33)\n",
        "- Dropped 5 highly correlated features\n",
        "- Added 4 binary indicator features\n",
        "- Class imbalance: DeFi_Trader 35% to Institutional 6%\n",
        "\n",
        "**Expected Model Performance (from research):**\n",
        "- 70-80% balanced accuracy\n",
        "- 65-75% macro F1\n",
        "- CatBoost with `auto_class_weights='Balanced'` recommended\n",
        "\n",
        "**Next:** Run Notebook 02 for model training"
      ]
    }
  ]
}
