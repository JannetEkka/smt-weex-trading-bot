{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SMT-WEEX Notebook 3: Evaluation & Insights\n",
        "**Project:** smt-weex-2025\n",
        "**Author:** Jannet Ekka\n",
        "\n",
        "This notebook:\n",
        "1. Deep evaluation of all models\n",
        "2. Confusion matrices\n",
        "3. Per-class performance\n",
        "4. Feature importance analysis\n",
        "5. Error analysis\n",
        "6. Trading signal insights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q catboost xgboost lightgbm scikit-learn pandas numpy matplotlib seaborn shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "PROJECT_ID = 'smt-weex-2025'\n",
        "BUCKET = 'smt-weex-2025-models'\n",
        "\n",
        "!gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    classification_report, confusion_matrix, roc_auc_score,\n",
        "    precision_recall_curve, average_precision_score, balanced_accuracy_score\n",
        ")\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Models and Data from GCS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download from GCS\n",
        "!mkdir -p /content/models\n",
        "!gsutil -m cp gs://{BUCKET}/models/initial/* /content/models/\n",
        "!gsutil cp gs://{BUCKET}/data/data_splits.npz /content/\n",
        "!gsutil cp gs://{BUCKET}/data/feature_config.json /content/\n",
        "!gsutil cp gs://{BUCKET}/data/whale_features_cleaned.csv /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data splits\n",
        "splits = np.load('/content/data_splits.npz')\n",
        "X_train, y_train = splits['X_train'], splits['y_train']\n",
        "X_val, y_val = splits['X_val'], splits['y_val']\n",
        "X_test, y_test = splits['X_test'], splits['y_test']\n",
        "\n",
        "# Load feature config\n",
        "with open('/content/feature_config.json', 'r') as f:\n",
        "    config = json.load(f)\n",
        "FEATURES = config['features']\n",
        "\n",
        "# Load label encoder\n",
        "with open('/content/models/label_encoder.pkl', 'rb') as f:\n",
        "    le = pickle.load(f)\n",
        "\n",
        "label_mapping = {i: label for i, label in enumerate(le.classes_)}\n",
        "labels = list(label_mapping.values())\n",
        "print(f\"Labels: {label_mapping}\")\n",
        "print(f\"Test set: {len(X_test)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load models\n",
        "models = {}\n",
        "\n",
        "# CatBoost\n",
        "models['CatBoost'] = CatBoostClassifier()\n",
        "models['CatBoost'].load_model('/content/models/catboost_whale_classifier.cbm')\n",
        "\n",
        "# Others\n",
        "with open('/content/models/xgboost_whale_classifier.pkl', 'rb') as f:\n",
        "    models['XGBoost'] = pickle.load(f)\n",
        "\n",
        "with open('/content/models/randomforest_whale_classifier.pkl', 'rb') as f:\n",
        "    models['RandomForest'] = pickle.load(f)\n",
        "\n",
        "with open('/content/models/lightgbm_whale_classifier.pkl', 'rb') as f:\n",
        "    models['LightGBM'] = pickle.load(f)\n",
        "\n",
        "print(f\"Loaded {len(models)} models\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Confusion Matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, labels, title):\n",
        "    \"\"\"Plot confusion matrix\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    \n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=labels, yticklabels=labels)\n",
        "    plt.title(title)\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot confusion matrices for all models\n",
        "confusion_matrices = {}\n",
        "for name, model in models.items():\n",
        "    y_pred = model.predict(X_test)\n",
        "    cm = plot_confusion_matrix(y_test, y_pred, labels, f'{name} Confusion Matrix')\n",
        "    confusion_matrices[name] = cm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Per-Class Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed classification report for best model (CatBoost)\n",
        "y_pred_catboost = models['CatBoost'].predict(X_test)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CatBoost Classification Report\")\n",
        "print(\"=\" * 60)\n",
        "print(classification_report(y_test, y_pred_catboost, target_names=labels, zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Per-class metrics comparison across models\n",
        "per_class_metrics = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    precision = precision_score(y_test, y_pred, average=None, zero_division=0)\n",
        "    recall = recall_score(y_test, y_pred, average=None, zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred, average=None, zero_division=0)\n",
        "    \n",
        "    per_class_metrics[model_name] = {\n",
        "        'precision': dict(zip(labels, precision)),\n",
        "        'recall': dict(zip(labels, recall)),\n",
        "        'f1': dict(zip(labels, f1))\n",
        "    }\n",
        "\n",
        "# Show per-class F1 for CatBoost\n",
        "print(\"\\n=== Per-Class F1 Scores (CatBoost) ===\")\n",
        "for label, f1_val in per_class_metrics['CatBoost']['f1'].items():\n",
        "    count = (y_test == le.transform([label])[0]).sum()\n",
        "    print(f\"{label:15s}: {f1_val:.4f} ({count} samples)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize per-class F1 across models\n",
        "fig, ax = plt.subplots(figsize=(14, 6))\n",
        "\n",
        "x = np.arange(len(labels))\n",
        "width = 0.2\n",
        "\n",
        "for i, (model_name, metrics) in enumerate(per_class_metrics.items()):\n",
        "    f1_values = [metrics['f1'][label] for label in labels]\n",
        "    ax.bar(x + i*width, f1_values, width, label=model_name)\n",
        "\n",
        "ax.set_ylabel('F1 Score')\n",
        "ax.set_title('Per-Class F1 Score Comparison')\n",
        "ax.set_xticks(x + width * 1.5)\n",
        "ax.set_xticklabels(labels, rotation=45, ha='right')\n",
        "ax.legend()\n",
        "ax.set_ylim(0, 1)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CatBoost feature importance\n",
        "catboost_importance = models['CatBoost'].get_feature_importance()\n",
        "importance_df = pd.DataFrame({\n",
        "    'feature': FEATURES,\n",
        "    'importance': catboost_importance\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"=== CatBoost Feature Importance (Top 15) ===\")\n",
        "print(importance_df.head(15).to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize feature importance\n",
        "plt.figure(figsize=(12, 10))\n",
        "top_n = 20\n",
        "top_features = importance_df.head(top_n)\n",
        "\n",
        "plt.barh(range(len(top_features)), top_features['importance'].values, color='steelblue')\n",
        "plt.yticks(range(len(top_features)), top_features['feature'].values)\n",
        "plt.xlabel('Importance')\n",
        "plt.title(f'Top {top_n} Most Important Features (CatBoost)')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare feature importance across models\n",
        "rf_importance = models['RandomForest'].feature_importances_\n",
        "xgb_importance = models['XGBoost'].feature_importances_\n",
        "\n",
        "importance_comparison = pd.DataFrame({\n",
        "    'feature': FEATURES,\n",
        "    'CatBoost': catboost_importance / catboost_importance.sum(),\n",
        "    'RandomForest': rf_importance / rf_importance.sum(),\n",
        "    'XGBoost': xgb_importance / xgb_importance.sum()\n",
        "})\n",
        "\n",
        "importance_comparison['avg'] = importance_comparison[['CatBoost', 'RandomForest', 'XGBoost']].mean(axis=1)\n",
        "importance_comparison = importance_comparison.sort_values('avg', ascending=False)\n",
        "\n",
        "print(\"=== Consensus Top Features (All Models) ===\")\n",
        "print(importance_comparison[['feature', 'avg', 'CatBoost', 'RandomForest', 'XGBoost']].head(15).to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Error Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze misclassifications\n",
        "y_pred = models['CatBoost'].predict(X_test)\n",
        "misclassified_idx = np.where(y_test != y_pred)[0]\n",
        "\n",
        "print(f\"Total misclassifications: {len(misclassified_idx)} / {len(y_test)} ({len(misclassified_idx)/len(y_test)*100:.1f}%)\")\n",
        "\n",
        "# Most common confusion pairs\n",
        "confusion_pairs = []\n",
        "for idx in misclassified_idx:\n",
        "    true_label = label_mapping[y_test[idx]]\n",
        "    pred_label = label_mapping[y_pred[idx]]\n",
        "    confusion_pairs.append((true_label, pred_label))\n",
        "\n",
        "from collections import Counter\n",
        "confusion_counts = Counter(confusion_pairs)\n",
        "\n",
        "print(\"\\n=== Most Common Misclassifications ===\")\n",
        "for (true_l, pred_l), count in confusion_counts.most_common(10):\n",
        "    print(f\"{true_l} -> {pred_l}: {count} times\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Low confidence predictions\n",
        "y_proba = models['CatBoost'].predict_proba(X_test)\n",
        "max_proba = y_proba.max(axis=1)\n",
        "\n",
        "print(\"\\n=== Prediction Confidence Distribution ===\")\n",
        "print(f\"Mean confidence: {max_proba.mean():.4f}\")\n",
        "print(f\"Min confidence: {max_proba.min():.4f}\")\n",
        "print(f\"Max confidence: {max_proba.max():.4f}\")\n",
        "\n",
        "# Low confidence threshold\n",
        "low_conf_threshold = 0.5\n",
        "low_conf_idx = max_proba < low_conf_threshold\n",
        "print(f\"\\nPredictions with confidence < {low_conf_threshold}: {low_conf_idx.sum()} ({low_conf_idx.sum()/len(y_test)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confidence distribution plot\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Overall confidence distribution\n",
        "axes[0].hist(max_proba, bins=20, color='steelblue', alpha=0.7, edgecolor='black')\n",
        "axes[0].axvline(x=0.5, color='red', linestyle='--', label='50% threshold')\n",
        "axes[0].set_xlabel('Confidence')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].set_title('Prediction Confidence Distribution')\n",
        "axes[0].legend()\n",
        "\n",
        "# Confidence by correct/incorrect\n",
        "correct_mask = y_test == y_pred\n",
        "axes[1].hist(max_proba[correct_mask], bins=20, alpha=0.7, label='Correct', color='green')\n",
        "axes[1].hist(max_proba[~correct_mask], bins=20, alpha=0.7, label='Incorrect', color='red')\n",
        "axes[1].set_xlabel('Confidence')\n",
        "axes[1].set_ylabel('Count')\n",
        "axes[1].set_title('Confidence: Correct vs Incorrect Predictions')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Trading Signal Insights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Key insights for trading signals\n",
        "print(\"=\" * 60)\n",
        "print(\"TRADING SIGNAL INSIGHTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Category-specific feature patterns\n",
        "df_full = pd.read_csv('/content/whale_features_cleaned.csv')\n",
        "\n",
        "# Features most relevant for trading signals\n",
        "signal_features = ['net_flow_eth_signed_log', 'tx_ratio_out_in', 'defi_interactions', 'cex_interactions', 'erc20_ratio']\n",
        "available_signal_features = [f for f in signal_features if f in df_full.columns]\n",
        "\n",
        "print(\"\\n=== Category Feature Patterns (for signals) ===\")\n",
        "category_patterns = df_full.groupby('category')[available_signal_features].agg(['mean', 'std']).round(4)\n",
        "print(category_patterns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Signal logic based on classification\n",
        "print(\"\\n=== Trading Signal Logic ===\")\n",
        "print(\"\"\"\n",
        "Based on model's feature importance and category patterns:\n",
        "\n",
        "1. CEX_Wallet:\n",
        "   - High incoming_count + low defi_interactions = Exchange deposit hub\n",
        "   - Signal: Large inflow spike = BEARISH (whales depositing to sell)\n",
        "   - Signal: Large outflow spike = BULLISH (whales withdrawing to hold)\n",
        "\n",
        "2. DeFi_Trader:\n",
        "   - High erc20_ratio + high defi_interactions + high internal_ratio\n",
        "   - Signal: Sudden protocol exit = VOLATILITY WARNING\n",
        "   - Signal: Large DEX swap = Follow direction (copy trade)\n",
        "\n",
        "3. Staker:\n",
        "   - Low tx_per_day + interactions with Lido/RocketPool\n",
        "   - Signal: Unstaking = BEARISH (need liquidity, might sell)\n",
        "   - Signal: Staking = BULLISH (long-term commitment)\n",
        "\n",
        "4. Miner:\n",
        "   - High outgoing_count + low erc20_ratio + regular timing\n",
        "   - Signal: Selling acceleration = BEARISH\n",
        "   - Signal: Accumulating (not selling) = BULLISH\n",
        "\n",
        "5. Institutional:\n",
        "   - High max_tx_value + business hours activity\n",
        "   - Signal: Follow their direction (they have alpha)\n",
        "   - Large buy = BULLISH, Large sell = BEARISH\n",
        "\n",
        "6. Exploiter:\n",
        "   - Unusual patterns, high gas, rapid movement\n",
        "   - Signal: AVOID - ignore their movements for trading\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Evaluation Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert confusion_counts to serializable format\n",
        "confusion_counts_dict = {f\"{k[0]}->{k[1]}\": v for k, v in confusion_counts.items()}\n",
        "\n",
        "# Save all evaluation results\n",
        "evaluation_results = {\n",
        "    'per_class_metrics': {k: {metric: {str(label): float(val) for label, val in values.items()} \n",
        "                              for metric, values in v.items()} \n",
        "                          for k, v in per_class_metrics.items()},\n",
        "    'feature_importance': importance_df.to_dict(orient='records'),\n",
        "    'confusion_pairs': confusion_counts_dict,\n",
        "    'best_model': 'CatBoost',\n",
        "    'timestamp': str(pd.Timestamp.now())\n",
        "}\n",
        "\n",
        "with open('/content/evaluation_results.json', 'w') as f:\n",
        "    json.dump(evaluation_results, f, indent=2, default=str)\n",
        "\n",
        "!gsutil cp /content/evaluation_results.json gs://{BUCKET}/results/evaluation_results.json\n",
        "print(\"Evaluation results saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Evaluation completed:\n",
        "1. Confusion matrices for all models\n",
        "2. Per-class performance analysis\n",
        "3. Feature importance ranking\n",
        "4. Error analysis (misclassification patterns)\n",
        "5. Trading signal insights\n",
        "\n",
        "**Key Findings:**\n",
        "- Top features: [See output above]\n",
        "- Hardest classes: [See output above]\n",
        "- Most common confusions: [See output above]\n",
        "\n",
        "**Next:** Run Notebook 4 for hyperparameter tuning with RandomSearch."
      ]
    }
  ]
}
