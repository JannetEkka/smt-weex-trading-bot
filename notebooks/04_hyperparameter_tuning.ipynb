{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SMT-WEEX Notebook 4: Hyperparameter Tuning\n",
        "**Project:** smt-weex-2025\n",
        "**Author:** Jannet Ekka\n",
        "\n",
        "**Focus:**\n",
        "- Tune the **5-class merged model** (best from notebook 03)\n",
        "- RandomizedSearchCV for CatBoost\n",
        "- Investigate Exploiter detection drop\n",
        "- Export production model\n",
        "\n",
        "**Baseline from Notebook 03:**\n",
        "- 5-class CV F1: 63.6% (+/- 1.0%)\n",
        "- Target: 65-70% F1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q catboost scikit-learn pandas numpy matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "PROJECT_ID = 'smt-weex-2025'\n",
        "BUCKET = 'smt-weex-2025-models'\n",
        "\n",
        "!gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split, StratifiedKFold, RandomizedSearchCV, cross_val_score\n",
        ")\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import (\n",
        "    f1_score, accuracy_score, classification_report, \n",
        "    confusion_matrix, make_scorer\n",
        ")\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Data and Prepare Merged Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download data\n",
        "!gsutil cp gs://{BUCKET}/data/whale_features_cleaned.csv /content/\n",
        "!gsutil cp gs://{BUCKET}/data/feature_config.json /content/\n",
        "\n",
        "df = pd.read_csv('/content/whale_features_cleaned.csv')\n",
        "\n",
        "with open('/content/feature_config.json', 'r') as f:\n",
        "    config = json.load(f)\n",
        "FEATURES = config['features']\n",
        "\n",
        "print(f\"Loaded {len(df)} samples, {len(FEATURES)} features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create merged dataset (from notebook 03)\n",
        "# Merge Institutional + CEX_Wallet -> Large_Holder\n",
        "df['category_merged'] = df['category'].replace({\n",
        "    'Institutional': 'Large_Holder',\n",
        "    'CEX_Wallet': 'Large_Holder'\n",
        "})\n",
        "\n",
        "print(\"=== Merged Class Distribution ===\")\n",
        "print(df['category_merged'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare X and y\n",
        "X = df[FEATURES].values\n",
        "y_raw = df['category_merged'].values\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y_raw)\n",
        "\n",
        "labels = list(le.classes_)\n",
        "n_classes = len(labels)\n",
        "\n",
        "print(f\"Labels: {labels}\")\n",
        "print(f\"Classes: {n_classes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train/test split (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.20,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Train: {len(X_train)}, Test: {len(X_test)}\")\n",
        "print(f\"\\nTest distribution:\")\n",
        "for i, label in enumerate(labels):\n",
        "    print(f\"  {label}: {(y_test == i).sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Baseline Model (from Notebook 03)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_predictions(model, X):\n",
        "    \"\"\"Get predictions, handling CatBoost's 2D output\"\"\"\n",
        "    y_pred = model.predict(X)\n",
        "    if hasattr(y_pred, 'shape') and len(y_pred.shape) > 1:\n",
        "        y_pred = y_pred.flatten()\n",
        "    return y_pred.astype(int)\n",
        "\n",
        "def evaluate_model(model, X_test, y_test, labels):\n",
        "    \"\"\"Evaluate model and print results\"\"\"\n",
        "    y_pred = get_predictions(model, X_test)\n",
        "    \n",
        "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "    f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    \n",
        "    print(f\"Accuracy:     {accuracy:.4f}\")\n",
        "    print(f\"F1 Macro:     {f1_macro:.4f}\")\n",
        "    print(f\"F1 Weighted:  {f1_weighted:.4f}\")\n",
        "    print(\"\\nPer-class F1:\")\n",
        "    f1_per_class = f1_score(y_test, y_pred, average=None)\n",
        "    for label, f1 in zip(labels, f1_per_class):\n",
        "        print(f\"  {label:15s}: {f1:.4f}\")\n",
        "    \n",
        "    return f1_macro, f1_per_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train baseline model (same params as notebook 03)\n",
        "print(\"=\" * 60)\n",
        "print(\"BASELINE MODEL (from Notebook 03)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "baseline_model = CatBoostClassifier(\n",
        "    iterations=300,\n",
        "    learning_rate=0.03,\n",
        "    depth=5,\n",
        "    l2_leaf_reg=3,\n",
        "    loss_function='MultiClass',\n",
        "    random_seed=42,\n",
        "    verbose=50,\n",
        "    auto_class_weights='Balanced'\n",
        ")\n",
        "\n",
        "baseline_model.fit(X_train, y_train)\n",
        "print(\"\\n--- Baseline Results ---\")\n",
        "baseline_f1, baseline_per_class = evaluate_model(baseline_model, X_test, y_test, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Baseline 5-fold CV\n",
        "print(\"\\n=== Baseline 5-Fold CV ===\")\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "baseline_cv_scores = []\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "    cb_temp = CatBoostClassifier(\n",
        "        iterations=300, learning_rate=0.03, depth=5, l2_leaf_reg=3,\n",
        "        random_seed=42, verbose=0, auto_class_weights='Balanced'\n",
        "    )\n",
        "    cb_temp.fit(X[train_idx], y[train_idx])\n",
        "    y_pred = get_predictions(cb_temp, X[val_idx])\n",
        "    score = f1_score(y[val_idx], y_pred, average='macro')\n",
        "    baseline_cv_scores.append(score)\n",
        "    print(f\"Fold {fold+1}: {score:.4f}\")\n",
        "\n",
        "print(f\"\\nBaseline CV: {np.mean(baseline_cv_scores):.4f} (+/- {np.std(baseline_cv_scores):.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Investigate Exploiter Detection\n",
        "\n",
        "In notebook 03, Exploiter F1 dropped from 94% (6-class) to 53% (5-class merged).\n",
        "Let's understand why and fix it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check Exploiter confusion in merged model\n",
        "y_pred_baseline = get_predictions(baseline_model, X_test)\n",
        "\n",
        "# Get Exploiter index\n",
        "exploiter_idx = list(le.classes_).index('Exploiter')\n",
        "\n",
        "# Find all Exploiters in test set\n",
        "exploiter_mask = y_test == exploiter_idx\n",
        "exploiter_total = exploiter_mask.sum()\n",
        "exploiter_correct = (y_pred_baseline[exploiter_mask] == exploiter_idx).sum()\n",
        "\n",
        "print(f\"=== Exploiter Detection Analysis ===\")\n",
        "print(f\"Total Exploiters in test: {exploiter_total}\")\n",
        "print(f\"Correctly identified: {exploiter_correct}\")\n",
        "print(f\"Accuracy: {exploiter_correct/exploiter_total*100:.1f}%\")\n",
        "\n",
        "# What are Exploiters misclassified as?\n",
        "print(f\"\\nMisclassification breakdown:\")\n",
        "for i, label in enumerate(labels):\n",
        "    count = (y_pred_baseline[exploiter_mask] == i).sum()\n",
        "    if count > 0:\n",
        "        print(f\"  Predicted as {label}: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot confusion matrix for baseline\n",
        "cm = confusion_matrix(y_test, y_pred_baseline)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=labels, yticklabels=labels)\n",
        "plt.title('Baseline Model Confusion Matrix (5-class)')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Hyperparameter Tuning with RandomizedSearchCV\n",
        "\n",
        "### Parameter Ranges (based on research):\n",
        "- `iterations`: 200-500 (more iterations for small data)\n",
        "- `depth`: 4-7 (shallow to prevent overfitting)\n",
        "- `learning_rate`: 0.01-0.1\n",
        "- `l2_leaf_reg`: 1-10 (regularization)\n",
        "- `border_count`: 32-255 (split precision)\n",
        "- `bagging_temperature`: 0-1 (randomization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define parameter grid\n",
        "param_distributions = {\n",
        "    'iterations': [200, 250, 300, 350, 400, 500],\n",
        "    'depth': [4, 5, 6, 7],\n",
        "    'learning_rate': [0.01, 0.02, 0.03, 0.05, 0.07, 0.1],\n",
        "    'l2_leaf_reg': [1, 2, 3, 5, 7, 10],\n",
        "    'border_count': [32, 64, 128, 255],\n",
        "    'bagging_temperature': [0, 0.5, 1.0],\n",
        "    'random_strength': [0, 0.5, 1.0],\n",
        "}\n",
        "\n",
        "print(\"Parameter search space:\")\n",
        "total_combinations = 1\n",
        "for param, values in param_distributions.items():\n",
        "    print(f\"  {param}: {values}\")\n",
        "    total_combinations *= len(values)\n",
        "print(f\"\\nTotal possible combinations: {total_combinations}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create base model for RandomizedSearchCV\n",
        "base_model = CatBoostClassifier(\n",
        "    loss_function='MultiClass',\n",
        "    random_seed=42,\n",
        "    verbose=0,\n",
        "    auto_class_weights='Balanced',\n",
        "    thread_count=-1\n",
        ")\n",
        "\n",
        "# Custom scorer for macro F1\n",
        "f1_macro_scorer = make_scorer(f1_score, average='macro')\n",
        "\n",
        "# RandomizedSearchCV\n",
        "print(\"=\" * 60)\n",
        "print(\"RANDOMIZED SEARCH CV (This may take 10-20 minutes)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=base_model,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=50,  # Test 50 random combinations\n",
        "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
        "    scoring=f1_macro_scorer,\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    random_state=42,\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"\\nSearch complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Best parameters\n",
        "print(\"=\" * 60)\n",
        "print(\"BEST PARAMETERS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Best CV Score: {random_search.best_score_:.4f}\")\n",
        "print(f\"\\nBest Parameters:\")\n",
        "for param, value in random_search.best_params_.items():\n",
        "    print(f\"  {param}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Top 10 parameter combinations\n",
        "results_df = pd.DataFrame(random_search.cv_results_)\n",
        "results_df = results_df.sort_values('rank_test_score')\n",
        "\n",
        "print(\"\\n=== Top 10 Parameter Combinations ===\")\n",
        "top_10 = results_df[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']].head(10)\n",
        "for idx, row in top_10.iterrows():\n",
        "    print(f\"\\nRank {int(row['rank_test_score'])}: CV F1 = {row['mean_test_score']:.4f} (+/- {row['std_test_score']:.4f})\")\n",
        "    print(f\"  Params: {row['params']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Train Final Model with Best Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train final model with best parameters\n",
        "print(\"=\" * 60)\n",
        "print(\"TRAINING FINAL MODEL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "best_params = random_search.best_params_.copy()\n",
        "best_params['loss_function'] = 'MultiClass'\n",
        "best_params['random_seed'] = 42\n",
        "best_params['auto_class_weights'] = 'Balanced'\n",
        "best_params['verbose'] = 50\n",
        "\n",
        "final_model = CatBoostClassifier(**best_params)\n",
        "final_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nFinal model trained!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate final model on test set\n",
        "print(\"\\n=== Final Model Test Results ===\")\n",
        "final_f1, final_per_class = evaluate_model(final_model, X_test, y_test, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5-fold CV with best parameters for reliable estimate\n",
        "print(\"\\n=== Final Model 5-Fold CV ===\")\n",
        "final_cv_scores = []\n",
        "final_cv_per_class = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "    params = best_params.copy()\n",
        "    params['verbose'] = 0\n",
        "    \n",
        "    cb_temp = CatBoostClassifier(**params)\n",
        "    cb_temp.fit(X[train_idx], y[train_idx])\n",
        "    y_pred = get_predictions(cb_temp, X[val_idx])\n",
        "    \n",
        "    score = f1_score(y[val_idx], y_pred, average='macro')\n",
        "    per_class = f1_score(y[val_idx], y_pred, average=None)\n",
        "    \n",
        "    final_cv_scores.append(score)\n",
        "    final_cv_per_class.append(per_class)\n",
        "    print(f\"Fold {fold+1}: {score:.4f}\")\n",
        "\n",
        "print(f\"\\nFinal CV: {np.mean(final_cv_scores):.4f} (+/- {np.std(final_cv_scores):.4f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Per-class CV performance\n",
        "print(\"\\n=== Per-Class CV Performance ===\")\n",
        "final_cv_per_class_mean = np.mean(final_cv_per_class, axis=0)\n",
        "final_cv_per_class_std = np.std(final_cv_per_class, axis=0)\n",
        "\n",
        "for i, label in enumerate(labels):\n",
        "    print(f\"{label:15s}: {final_cv_per_class_mean[i]:.4f} (+/- {final_cv_per_class_std[i]:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Compare Baseline vs Tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparison\n",
        "print(\"=\" * 60)\n",
        "print(\"BASELINE vs TUNED MODEL COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\n{'Metric':<25} {'Baseline':<15} {'Tuned':<15} {'Improvement':<15}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Holdout\n",
        "print(f\"{'Holdout F1 Macro':<25} {baseline_f1:<15.4f} {final_f1:<15.4f} {(final_f1-baseline_f1)*100:+.2f}%\")\n",
        "\n",
        "# CV\n",
        "baseline_cv_mean = np.mean(baseline_cv_scores)\n",
        "final_cv_mean = np.mean(final_cv_scores)\n",
        "print(f\"{'CV F1 Macro (mean)':<25} {baseline_cv_mean:<15.4f} {final_cv_mean:<15.4f} {(final_cv_mean-baseline_cv_mean)*100:+.2f}%\")\n",
        "\n",
        "# CV std\n",
        "baseline_cv_std = np.std(baseline_cv_scores)\n",
        "final_cv_std = np.std(final_cv_scores)\n",
        "print(f\"{'CV F1 Std':<25} {baseline_cv_std:<15.4f} {final_cv_std:<15.4f} {(final_cv_std-baseline_cv_std)*100:+.2f}%\")\n",
        "\n",
        "# Per-class comparison\n",
        "print(\"\\n--- Per-Class F1 (Holdout) ---\")\n",
        "baseline_per_class_arr = np.array(baseline_per_class)\n",
        "final_per_class_arr = np.array(final_per_class)\n",
        "\n",
        "for i, label in enumerate(labels):\n",
        "    diff = (final_per_class_arr[i] - baseline_per_class_arr[i]) * 100\n",
        "    print(f\"{label:<15} {baseline_per_class_arr[i]:<15.4f} {final_per_class_arr[i]:<15.4f} {diff:+.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Overall comparison\n",
        "metrics = ['Holdout F1', 'CV Mean F1']\n",
        "baseline_vals = [baseline_f1, baseline_cv_mean]\n",
        "tuned_vals = [final_f1, final_cv_mean]\n",
        "\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "axes[0].bar(x - width/2, baseline_vals, width, label='Baseline', color='steelblue')\n",
        "axes[0].bar(x + width/2, tuned_vals, width, label='Tuned', color='coral')\n",
        "axes[0].set_ylabel('F1 Score')\n",
        "axes[0].set_title('Baseline vs Tuned Model')\n",
        "axes[0].set_xticks(x)\n",
        "axes[0].set_xticklabels(metrics)\n",
        "axes[0].legend()\n",
        "axes[0].set_ylim(0, 1)\n",
        "\n",
        "# Per-class comparison\n",
        "x = np.arange(len(labels))\n",
        "axes[1].bar(x - width/2, baseline_per_class_arr, width, label='Baseline', color='steelblue')\n",
        "axes[1].bar(x + width/2, final_per_class_arr, width, label='Tuned', color='coral')\n",
        "axes[1].set_ylabel('F1 Score')\n",
        "axes[1].set_title('Per-Class F1 Comparison')\n",
        "axes[1].set_xticks(x)\n",
        "axes[1].set_xticklabels(labels, rotation=45, ha='right')\n",
        "axes[1].legend()\n",
        "axes[1].set_ylim(0, 1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Final Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final model confusion matrix\n",
        "y_pred_final = get_predictions(final_model, X_test)\n",
        "cm = confusion_matrix(y_test, y_pred_final)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=labels, yticklabels=labels)\n",
        "plt.title('Final Tuned Model Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Classification report\n",
        "print(\"\\n=== Final Model Classification Report ===\")\n",
        "print(classification_report(y_test, y_pred_final, target_names=labels, zero_division=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Feature Importance (Tuned Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance\n",
        "importance = final_model.get_feature_importance()\n",
        "importance_df = pd.DataFrame({\n",
        "    'feature': FEATURES,\n",
        "    'importance': importance\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"=== Top 15 Features (Tuned Model) ===\")\n",
        "print(importance_df.head(15).to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize\n",
        "plt.figure(figsize=(12, 10))\n",
        "top_n = 20\n",
        "top_features = importance_df.head(top_n)\n",
        "\n",
        "plt.barh(range(len(top_features)), top_features['importance'].values, color='coral')\n",
        "plt.yticks(range(len(top_features)), top_features['feature'].values)\n",
        "plt.xlabel('Importance')\n",
        "plt.title(f'Top {top_n} Most Important Features (Tuned Model)')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Save Production Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs('/content/production', exist_ok=True)\n",
        "\n",
        "# Save final model\n",
        "final_model.save_model('/content/production/catboost_whale_classifier_production.cbm')\n",
        "\n",
        "# Save label encoder\n",
        "with open('/content/production/label_encoder_production.pkl', 'wb') as f:\n",
        "    pickle.dump(le, f)\n",
        "\n",
        "# Save feature list\n",
        "with open('/content/production/features.json', 'w') as f:\n",
        "    json.dump({'features': FEATURES}, f, indent=2)\n",
        "\n",
        "# Save model config and results\n",
        "model_config = {\n",
        "    'model_type': 'CatBoost',\n",
        "    'n_classes': n_classes,\n",
        "    'classes': labels,\n",
        "    'n_features': len(FEATURES),\n",
        "    'best_params': best_params,\n",
        "    'performance': {\n",
        "        'holdout_f1_macro': float(final_f1),\n",
        "        'cv_f1_macro_mean': float(final_cv_mean),\n",
        "        'cv_f1_macro_std': float(final_cv_std),\n",
        "        'per_class_f1': {label: float(f1) for label, f1 in zip(labels, final_per_class)}\n",
        "    },\n",
        "    'baseline_comparison': {\n",
        "        'baseline_cv_f1': float(baseline_cv_mean),\n",
        "        'tuned_cv_f1': float(final_cv_mean),\n",
        "        'improvement_pct': float((final_cv_mean - baseline_cv_mean) * 100)\n",
        "    },\n",
        "    'training_date': datetime.now().isoformat(),\n",
        "    'merged_classes': {\n",
        "        'original': ['Institutional', 'CEX_Wallet'],\n",
        "        'merged_to': 'Large_Holder'\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('/content/production/model_config.json', 'w') as f:\n",
        "    json.dump(model_config, f, indent=2)\n",
        "\n",
        "print(\"Production model saved locally\")\n",
        "print(json.dumps(model_config, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload to GCS\n",
        "!gsutil -m cp -r /content/production/* gs://{BUCKET}/models/production/\n",
        "print(f\"\\nUploaded to gs://{BUCKET}/models/production/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Also save search results for reference\n",
        "results_df[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']].to_csv(\n",
        "    '/content/production/tuning_results.csv', index=False\n",
        ")\n",
        "!gsutil cp /content/production/tuning_results.csv gs://{BUCKET}/models/production/\n",
        "print(\"Tuning results saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Production Usage Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: How to use the production model\n",
        "print(\"=\" * 60)\n",
        "print(\"PRODUCTION USAGE EXAMPLE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\"\"\n",
        "# Load model\n",
        "from catboost import CatBoostClassifier\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "# Download from GCS\n",
        "# gsutil cp gs://smt-weex-2025-models/models/production/* ./\n",
        "\n",
        "# Load\n",
        "model = CatBoostClassifier()\n",
        "model.load_model('catboost_whale_classifier_production.cbm')\n",
        "\n",
        "with open('label_encoder_production.pkl', 'rb') as f:\n",
        "    le = pickle.load(f)\n",
        "\n",
        "with open('features.json', 'r') as f:\n",
        "    features = json.load(f)['features']\n",
        "\n",
        "# Predict\n",
        "X_new = extract_features(wallet_address)  # Your feature extraction\n",
        "X_new = X_new[features].values.reshape(1, -1)\n",
        "\n",
        "prediction = model.predict(X_new)\n",
        "category = le.inverse_transform(prediction.flatten())[0]\n",
        "\n",
        "# Get probabilities\n",
        "probas = model.predict_proba(X_new)\n",
        "confidence = probas.max()\n",
        "\n",
        "print(f\"Category: {category}, Confidence: {confidence:.2%}\")\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### Results:\n",
        "- **Baseline (5-class):** CV F1 = [see output]\n",
        "- **Tuned (5-class):** CV F1 = [see output]\n",
        "- **Improvement:** [see output]\n",
        "\n",
        "### Best Parameters:\n",
        "[see output above]\n",
        "\n",
        "### Production Model:\n",
        "- Saved to: `gs://smt-weex-2025-models/models/production/`\n",
        "- Files:\n",
        "  - `catboost_whale_classifier_production.cbm`\n",
        "  - `label_encoder_production.pkl`\n",
        "  - `features.json`\n",
        "  - `model_config.json`\n",
        "\n",
        "### Per-Class Reliability (for Trading Signals):\n",
        "| Category | F1 | Trading Action |\n",
        "|----------|-----|----------------|\n",
        "| Miner | [see output] | BEARISH on sells |\n",
        "| DeFi_Trader | [see output] | FOLLOW with caution |\n",
        "| Large_Holder | [see output] | Exchange flow |\n",
        "| Staker | [see output] | Weak unstake signal |\n",
        "| Exploiter | [see output] | AVOID |\n",
        "\n",
        "### Next Steps:\n",
        "1. Integrate model into FastAPI backend\n",
        "2. Set up real-time inference pipeline\n",
        "3. Monitor model performance in production"
      ]
    }
  ]
}
